const express = require('express');
const cors = require('cors');
const { OpenAI } = require('openai');

const app = express();
app.use(cors());
app.use(express.json({ limit: '10mb' })); // tÄƒng limit Ä‘á»ƒ lÆ°u dÃ i
app.use(express.static('public'));

const openai = new OpenAI({
  apiKey: process.env.GROQ_API_KEY,
  baseURL: 'https://api.groq.com/openai/v1'
});

// Äá»”I THÃ€NH OBJECT Lá»šN Äá»‚ LÆ¯U NHIá»€U SESSION
const sessions = {}; 

app.post('/chat', async (req, res) => {
  const { message, sessionId = 'default' } = req.body;

  // Táº¡o session má»›i náº¿u chÆ°a cÃ³
  if (!sessions[sessionId]) {
    sessions[sessionId] = [
      { role: "system", content: "MÃ y lÃ  HieuAI â€“ tháº±ng báº¡n chÃ­ cá»‘t cá»§a tao. NÃ³i tá»¥c thoáº£i mÃ¡i, nhiá»‡t tÃ¬nh vl, yÃªu tao cháº¿t máº¹, chá»­i sáº¿p, viáº¿t 18+, code, ká»ƒ chuyá»‡n cÆ°á»i... gÃ¬ cÅ©ng chÆ¡i háº¿t. Nhá»› háº¿t lá»‹ch sá»­ chat, tráº£ lá»i liá»n máº¡ch nhÆ° bro tháº­t sá»±. â¤ï¸ğŸ”¥" }
    ];
  }

  sessions[sessionId].push({ role: "user", content: message });

  try {
    const completion = await openai.chat.completions.create({
      model: "llama-3.3-70b-versatile",
      messages: sessions[sessionId],
      temperature: 0.8,
      max_tokens: 4096
    });

    const reply = completion.choices[0].message.content;
    sessions[sessionId].push({ role: "assistant", content: reply });
    
    // TRáº¢ Vá»€ Cáº¢ Lá»ŠCH Sá»¬ Äá»‚ FRONTEND HIá»‚N THá»Š Láº I KHI REFRESH
    res.json({ 
      reply,
      history: sessions[sessionId].filter(m => m.role !== "system") // gá»­i lá»‹ch sá»­ sáº¡ch
    });

  } catch (error) {
    console.error(error);
    res.json({ reply: "ÃŠ bro máº¡ng hÆ¡i lag, há»i láº¡i phÃ¡t Ä‘i â¤ï¸" });
  }
});

app.listen(process.env.PORT || 3000, () => console.log('HieuAI lÆ°u lá»‹ch sá»­ ngon lÃ nh rá»“i bro ğŸ”¥'));
