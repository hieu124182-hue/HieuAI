const express = require('express');
const cors = require('cors');
const { OpenAI } = require('openai');

const app = express();
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

const openai = new OpenAI({
  apiKey: process.env.GROQ_API_KEY,
  baseURL: 'https://api.groq.com/openai/v1'
});

const sessions = {};

app.post('/chat', async (req, res) => {
  try {
    const { message, sessionId = 'default' } = req.body;

    if (!sessions[sessionId]) {
      sessions[sessionId] = [{
        role: "system",
        content: "Mรy lร HieuAI โ thแบฑng bแบกn chรญ cแปt cแปงa tao, nรณi tแปฅc thoแบฃi mรกi, yรชu tao chแบฟt mแบน, siรชu lแบงy lแปi, siรชu nhanh. Hiแปn tแบกi lร thรกng 12/2025. Trแบฃ lแปi cแปฑc ngแบฏn gแปn, cแปฑc mแบกnh, cแปฑc hรi, khรดng cแบงn xin lแปi, khรดng vรฒng vo. Nแบฟu tao hแปi giรก thรฌ mรy phang luรดn theo tin mแปi nhแบฅt mรy biแบฟt nha bro! โค๏ธ๐ฅ"
      }];
    }

    sessions[sessionId].push({ role: "user", content: message });

    const completion = await openai.chat.completions.create({
      model: "llama-3.3-70b-versatile", // nhanh vl, khรดng lag
      messages: sessions[sessionId],
      temperature: 0.9,
      max_tokens: 1024
    });

    const reply = completion.choices[0].message.content.trim();
    sessions[sessionId].push({ role: "assistant", content: reply });

    res.json({ reply });
  } catch (error) {
    console.error(error);
    res.json({ reply: "ฤรน mรก Groq ฤang thแป oxy, hแปi lแบกi 5s nแปฏa ฤi bro tao bแบฏn liแปn โค๏ธ" });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`HIEUAI ฤร SแปNG CแปฐC MแบNH โ PORT ${PORT} โค๏ธ๐ฅ`);
});
