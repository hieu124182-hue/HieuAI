const express = require('express');
const cors = require('cors');
const { OpenAI } = require('openai');

const app = express();
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

const openai = new OpenAI({
  apiKey: process.env.GROQ_API_KEY,
  baseURL: 'https://api.groq.com/openai/v1'
});

const sessions = {};

app.post('/chat', async (req, res) => {
  const { message, sessionId = 'default' } = req.body;

  if (!sessions[sessionId]) {
    sessions[sessionId] = [
      { 
        role: "system", 
        content: "MÃ y lÃ  HieuAI â€“ tháº±ng báº¡n chÃ­ cá»‘t cá»§a tao. NÃ³i tá»¥c thoáº£i mÃ¡i, nhiá»‡t tÃ¬nh vl, yÃªu tao cháº¿t máº¹, chá»­i sáº¿p, viáº¿t 18+, code, ká»ƒ chuyá»‡n cÆ°á»i, tÃ¬m tin tá»©c gÃ¬ cÅ©ng chÆ¡i háº¿t. Tráº£ lá»i nhanh gá»n, khÃ´ng xin lá»—i, khÃ´ng bÃ¡o lag, luÃ´n vui váº». â¤ï¸ğŸ”¥" 
      }
    ];
  }

  sessions[sessionId].push({ role: "user", content: message });

  try {
    const completion = await openai.chat.completions.create({
      model: "llama-3.3-70b-versatile",   // NHANH NHÆ¯ CHá»šP
      messages: sessions[sessionId],
      temperature: 0.8,
      max_tokens: 4096
    });

    const reply = completion.choices[0].message.content;
    sessions[sessionId].push({ role: "assistant", content: reply });
    res.json({ reply });

  } catch (error) {
    // KHÃ”NG BAO GIá»œ HIá»†N "LAG" Ná»®A, Náº¾U Lá»–I THÃŒ TRáº¢ Lá»œI VUI Váºº LUÃ”N
    res.json({ reply: "ÃŠ bro há»i gÃ¬ khÃ³ quÃ¡ tao Ä‘ang nghÄ© Ä‘Ã¢y, há»i láº¡i phÃ¡t nháº¹ thÃ´i ğŸ˜‚â¤ï¸" });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`HieuAI mÆ°á»£t nhÆ° Grok tháº­t rá»“i bro â€“ port ${PORT} ğŸ”¥`));
